{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pinecone-client\n",
    "#!pip install llama-index-llms-gemini\n",
    "#!pip install llama-index-vector-stores-pinecone\n",
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import libraries and define API keys\n",
    "We'll need to import a few libraries and take care of some basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahuljoshi/Desktop/DataScienceProblems/endtoendMLProjects/genai/ragbasedchatbot/venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core import ServiceContext, VectorStoreIndex, SimpleDirectoryReader, download_loader, set_global_service_context\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set API keys and set Gemini as llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyDEvLkqFWDGcNEdfej5nGtGk_gqELwini4\"\n",
    "PINECONE_API_KEY = \"953c33e9-4c8e-4c61-868a-64b246640ef4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set llm as Gemini Pro\n",
    "llm = Gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a Pinecone client\n",
    "To send data back and forth between the app and Pinecone, we'll need to instantiate a Pinecone client. It's a one-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pinecone_client = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "pinecone_client = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testindex\n"
     ]
    }
   ],
   "source": [
    "# list pinecone indexes\n",
    "for index in pinecone_client.list_indexes():\n",
    "    print(index['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Select the Pinecone index\n",
    "Using our Pinecone client, we can select the Index that we previously created and assign it to the variable pinecone_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone_client.Index(\"testindex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Call the documents\n",
    "\n",
    "\n",
    "This is known as the ingestion phase. That's when LlamaIndex grabs the data and then converts it into chunks using the load_data method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Generate embeddings using GeminiEmbedding\n",
    "\n",
    "By default, LlamaIndex assumes you are using OpenAI to generate embeddings.\n",
    "To configure it to use Gemini instead, we need to set up the service context which lets LlamaIndex know which llm and which embedding model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Generate and store embeddings in the Pinecone index\n",
    "\n",
    "Using the VectorStoreIndex class, LlamaIndex takes care of sending the data chunks to the embedding model and then handles storing the vectorized data into the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 32/32 [00:01<00:00, 20.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# store embeddings in pinecone index\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "# Create a StorageContext using the created PineconeVectorStore\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "# Use the chunks of documents and the storage_context to create the index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "#Alternatively, if you want to load your existing index you can use the from_vector_store method of the VectorStoreIndex class as follows:\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Query Pinecone vector store\n",
    "\n",
    "Now the contents of the URL are converted to embeddings and stored in the Pinecone index.\n",
    "Let's perform a similarity search by querying the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query pinecone index for similar embeddings\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(query_engine,open('query.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_engine_1 = pickle.load(open('query.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'response_gen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gemini_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are the plans covered under Rooftop solarization and muft bijli?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_gen\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Response' object has no attribute 'response_gen'"
     ]
    }
   ],
   "source": [
    "gemini_response = query_engine.query(\"What are the plans covered under Rooftop solarization and muft bijli?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Through rooftop solarization, one crore households will be enabled to obtain up to 300 units free electricity every month.\n"
     ]
    }
   ],
   "source": [
    "# print response\n",
    "print(gemini_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.base.response.schema.Response"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gemini_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
